{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Victoria_G/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Forecast\n",
    "\n",
    "## Part I. Feature engineering\n",
    "\n",
    "The goal of the second part of the project is to use the previous data to predict the peak in advance. Currently, we have the monthwide average energy consumption for each time interval during a day. This dataset was generated from the last peak analysis part. The name of the data is \"monthwide_average.csv\".  We begin our feature engineering step by clarifying some assumptions.\n",
    "## 1. Assumptions on generating the feature space:\n",
    "<ol>\n",
    "<li> Since we want to predict the peak in advance (at least 15 min), the main idea of generating our feature space is that we want to use the data before the current time interval as features. In this situation, we will assume the data during the past one hour has impact on predicting the whether there will be a peak in the current interval.</li>\n",
    "<li> We assume the weather conditions have impact on whether there will be peaks during the current interval. And all the weather data we got could be obtained before the current time interval point. For example, the 8:00 weather condition information could be obtained before 8:00. The reason for this assumption is that we want to make sure that all the data in our feature space is the \"seen\" data, which we have already had and not including any unknown data.</li>\n",
    "</ol>\n",
    "  \n",
    "## 2. Components of the feature space:\n",
    "<ol>\n",
    "<li> Since we decide to use past one hour data to predict the peak, some of the features in our feature space should the past 4 time interval average energy use for each circuit. </li>\n",
    "<li> When doing **time series analysis**, the **historical change** of the variables are often very important. So we want to also add the change of the average energy for each circuit as important features. In addition, we also want to add the features such as the average energy use for each circuit during the past one hour.</li>\n",
    "<li> The third part of our fearure space would be the weather information for that time. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Add the past 1 hour data into our feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour_min</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X69</th>\n",
       "      <th>X70</th>\n",
       "      <th>X71</th>\n",
       "      <th>X72</th>\n",
       "      <th>Label</th>\n",
       "      <th>hour</th>\n",
       "      <th>min</th>\n",
       "      <th>year_mon</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2569.999878</td>\n",
       "      <td>2779.999817</td>\n",
       "      <td>1769.999969</td>\n",
       "      <td>1521.445237</td>\n",
       "      <td>1378.346362</td>\n",
       "      <td>2132.499939</td>\n",
       "      <td>2814.999939</td>\n",
       "      <td>2770.000000</td>\n",
       "      <td>332.999985</td>\n",
       "      <td>...</td>\n",
       "      <td>26474.998535</td>\n",
       "      <td>27224.998535</td>\n",
       "      <td>26899.999512</td>\n",
       "      <td>2624.999939</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160700</td>\n",
       "      <td>7</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>2569.999878</td>\n",
       "      <td>2772.133468</td>\n",
       "      <td>1767.535860</td>\n",
       "      <td>1513.517472</td>\n",
       "      <td>1384.441351</td>\n",
       "      <td>2146.986023</td>\n",
       "      <td>2817.765930</td>\n",
       "      <td>2745.710136</td>\n",
       "      <td>333.002252</td>\n",
       "      <td>...</td>\n",
       "      <td>26158.991927</td>\n",
       "      <td>26925.121973</td>\n",
       "      <td>26569.421354</td>\n",
       "      <td>2607.673588</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>20160700</td>\n",
       "      <td>7</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>2569.999878</td>\n",
       "      <td>2762.499939</td>\n",
       "      <td>1471.119179</td>\n",
       "      <td>1268.127825</td>\n",
       "      <td>1205.714195</td>\n",
       "      <td>2147.499878</td>\n",
       "      <td>2819.999939</td>\n",
       "      <td>2722.836491</td>\n",
       "      <td>332.999985</td>\n",
       "      <td>...</td>\n",
       "      <td>22015.902421</td>\n",
       "      <td>22685.226270</td>\n",
       "      <td>22404.523958</td>\n",
       "      <td>2604.999939</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>20160700</td>\n",
       "      <td>7</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>2571.323625</td>\n",
       "      <td>2762.499939</td>\n",
       "      <td>1715.946231</td>\n",
       "      <td>1484.120264</td>\n",
       "      <td>1362.346784</td>\n",
       "      <td>2149.033541</td>\n",
       "      <td>2819.999939</td>\n",
       "      <td>2710.000000</td>\n",
       "      <td>333.177592</td>\n",
       "      <td>...</td>\n",
       "      <td>25749.999023</td>\n",
       "      <td>26549.998535</td>\n",
       "      <td>26174.999023</td>\n",
       "      <td>2604.999939</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>20160700</td>\n",
       "      <td>7</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>2574.999817</td>\n",
       "      <td>2762.499939</td>\n",
       "      <td>1401.334080</td>\n",
       "      <td>1200.172838</td>\n",
       "      <td>1162.360573</td>\n",
       "      <td>2149.999817</td>\n",
       "      <td>2819.999939</td>\n",
       "      <td>2711.592212</td>\n",
       "      <td>333.249992</td>\n",
       "      <td>...</td>\n",
       "      <td>20893.533801</td>\n",
       "      <td>21533.086019</td>\n",
       "      <td>21265.799626</td>\n",
       "      <td>2604.999939</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20160700</td>\n",
       "      <td>7</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hour_min           X2           X3           X4           X5           X6  \\\n",
       "0         0  2569.999878  2779.999817  1769.999969  1521.445237  1378.346362   \n",
       "1        15  2569.999878  2772.133468  1767.535860  1513.517472  1384.441351   \n",
       "2        30  2569.999878  2762.499939  1471.119179  1268.127825  1205.714195   \n",
       "3        45  2571.323625  2762.499939  1715.946231  1484.120264  1362.346784   \n",
       "4       100  2574.999817  2762.499939  1401.334080  1200.172838  1162.360573   \n",
       "\n",
       "            X7           X8           X9         X10  ...            X69  \\\n",
       "0  2132.499939  2814.999939  2770.000000  332.999985  ...   26474.998535   \n",
       "1  2146.986023  2817.765930  2745.710136  333.002252  ...   26158.991927   \n",
       "2  2147.499878  2819.999939  2722.836491  332.999985  ...   22015.902421   \n",
       "3  2149.033541  2819.999939  2710.000000  333.177592  ...   25749.999023   \n",
       "4  2149.999817  2819.999939  2711.592212  333.249992  ...   20893.533801   \n",
       "\n",
       "            X70           X71          X72  Label  hour  min  year_mon  month  \\\n",
       "0  27224.998535  26899.999512  2624.999939      0     0    0  20160700      7   \n",
       "1  26925.121973  26569.421354  2607.673588      0     0   15  20160700      7   \n",
       "2  22685.226270  22404.523958  2604.999939      0     0   30  20160700      7   \n",
       "3  26549.998535  26174.999023  2604.999939      0     0   45  20160700      7   \n",
       "4  21533.086019  21265.799626  2604.999939      0     1    0  20160700      7   \n",
       "\n",
       "   year  \n",
       "0  2016  \n",
       "1  2016  \n",
       "2  2016  \n",
       "3  2016  \n",
       "4  2016  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the monthwide average data\n",
    "# Note: X2, X3,...X72 represent the 71 circuits\n",
    "\n",
    "data_monthwide= pd.read_csv(\"monthwide_average.csv\")\n",
    "data_monthwide.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** \n",
    "1. For each circuit, we want to generate 4 feature vectors. \n",
    "2. The past 1h, past 45 min, past 30 min, and past 15 min energy use.\n",
    "3. We should exclude the first 4 time points because there are not enough past data for these 4 points.\n",
    "4. So the final matix generated in this step will have **668(672-4) rows**, and **71* 4 = 284 columns**.\n",
    "5. 71 * 4 represents each circuit will have 4 past points recorded and there are 71 circuits in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct a empty matix to hold new features\n",
    "past_1h_matrix=np.zeros((668,284))\n",
    "\n",
    "# the precious circuit data matix\n",
    "circuit_matrix=data_monthwide.ix[:,1:-4].values\n",
    "\n",
    "# create the feature space\n",
    "i=0\n",
    "j=0\n",
    "while i < 284 and j < 71:\n",
    "    past_1h_matrix[:,i]=circuit_matrix[:-4,j]\n",
    "    past_1h_matrix[:,i+1]=circuit_matrix[1:-3,j]\n",
    "    past_1h_matrix[:,i+2]=circuit_matrix[2:-2,j]\n",
    "    past_1h_matrix[:,i+3]=circuit_matrix[3:-1,j]\n",
    "    i=i+4\n",
    "    j+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Add historical change & average for each time interval point during past 1 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct a empty matrix to hold the \"change & average over an hour\" variable\n",
    "# The dimension is 668 * 142 because we want to hold 71 \"change\" variable for each circuit and 71 \"average\" variable\n",
    "change_1h = np.zeros((668,142))\n",
    "i=0\n",
    "j=0\n",
    "k=0\n",
    "while i <72 and j < 284 and k<284:\n",
    "    change_1h[:,i]= np.absolute(past_1h_matrix[:,k]- past_1h_matrix[:,k+3])\n",
    "    change_1h[:,i+71]=np.mean(past_1h_matrix[:,j:j+4], axis=1)\n",
    "    i+=1\n",
    "    j=j+4\n",
    "    k=k+4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_mon</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>C1_1h</th>\n",
       "      <th>C1_45m</th>\n",
       "      <th>C1_30m</th>\n",
       "      <th>C1_15m</th>\n",
       "      <th>C2_1h</th>\n",
       "      <th>...</th>\n",
       "      <th>C62_avg</th>\n",
       "      <th>C63_avg</th>\n",
       "      <th>C64_avg</th>\n",
       "      <th>C65_avg</th>\n",
       "      <th>C66_avg</th>\n",
       "      <th>C67_avg</th>\n",
       "      <th>C68_avg</th>\n",
       "      <th>C69_avg</th>\n",
       "      <th>C70_avg</th>\n",
       "      <th>C71_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20160700</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.323747</td>\n",
       "      <td>17.499878</td>\n",
       "      <td>54.053738</td>\n",
       "      <td>37.324973</td>\n",
       "      <td>15.999578</td>\n",
       "      <td>...</td>\n",
       "      <td>22685.226270</td>\n",
       "      <td>26549.998535</td>\n",
       "      <td>26899.999512</td>\n",
       "      <td>26569.421354</td>\n",
       "      <td>22404.523958</td>\n",
       "      <td>26174.999023</td>\n",
       "      <td>2624.999939</td>\n",
       "      <td>2607.673588</td>\n",
       "      <td>2604.999939</td>\n",
       "      <td>2604.999939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20160700</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>4.999939</td>\n",
       "      <td>9.633529</td>\n",
       "      <td>366.201780</td>\n",
       "      <td>313.344634</td>\n",
       "      <td>222.080779</td>\n",
       "      <td>...</td>\n",
       "      <td>26549.998535</td>\n",
       "      <td>21533.086019</td>\n",
       "      <td>26569.421354</td>\n",
       "      <td>22404.523958</td>\n",
       "      <td>26174.999023</td>\n",
       "      <td>21265.799626</td>\n",
       "      <td>2607.673588</td>\n",
       "      <td>2604.999939</td>\n",
       "      <td>2604.999939</td>\n",
       "      <td>2604.999939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20160700</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>4.999939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>161.285451</td>\n",
       "      <td>145.532020</td>\n",
       "      <td>91.639445</td>\n",
       "      <td>...</td>\n",
       "      <td>21533.086019</td>\n",
       "      <td>20740.028044</td>\n",
       "      <td>22404.523958</td>\n",
       "      <td>26174.999023</td>\n",
       "      <td>21265.799626</td>\n",
       "      <td>20437.985954</td>\n",
       "      <td>2604.999939</td>\n",
       "      <td>2604.999939</td>\n",
       "      <td>2604.999939</td>\n",
       "      <td>2604.999939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20160700</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>3.374870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>338.933895</td>\n",
       "      <td>264.666146</td>\n",
       "      <td>179.289524</td>\n",
       "      <td>...</td>\n",
       "      <td>20740.028044</td>\n",
       "      <td>21725.356083</td>\n",
       "      <td>26174.999023</td>\n",
       "      <td>21265.799626</td>\n",
       "      <td>20437.985954</td>\n",
       "      <td>21374.249394</td>\n",
       "      <td>2604.999939</td>\n",
       "      <td>2604.999939</td>\n",
       "      <td>2604.999939</td>\n",
       "      <td>2604.999939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20160700</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7.499939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.474115</td>\n",
       "      <td>98.481982</td>\n",
       "      <td>57.822576</td>\n",
       "      <td>...</td>\n",
       "      <td>21725.356083</td>\n",
       "      <td>19803.037988</td>\n",
       "      <td>21265.799626</td>\n",
       "      <td>20437.985954</td>\n",
       "      <td>21374.249394</td>\n",
       "      <td>19502.152584</td>\n",
       "      <td>2604.999939</td>\n",
       "      <td>2604.999939</td>\n",
       "      <td>2604.999939</td>\n",
       "      <td>2604.999939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 431 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year_mon  year  month  hour  minute     C1_1h     C1_45m      C1_30m  \\\n",
       "0  20160700  2016      7     1       0  1.323747  17.499878   54.053738   \n",
       "1  20160700  2016      7     1      15  4.999939   9.633529  366.201780   \n",
       "2  20160700  2016      7     1      30  4.999939   0.000000  161.285451   \n",
       "3  20160700  2016      7     1      45  3.374870   0.000000  338.933895   \n",
       "4  20160700  2016      7     2       0  7.499939   0.000000  118.474115   \n",
       "\n",
       "       C1_15m       C2_1h     ...            C62_avg       C63_avg  \\\n",
       "0   37.324973   15.999578     ...       22685.226270  26549.998535   \n",
       "1  313.344634  222.080779     ...       26549.998535  21533.086019   \n",
       "2  145.532020   91.639445     ...       21533.086019  20740.028044   \n",
       "3  264.666146  179.289524     ...       20740.028044  21725.356083   \n",
       "4   98.481982   57.822576     ...       21725.356083  19803.037988   \n",
       "\n",
       "        C64_avg       C65_avg       C66_avg       C67_avg      C68_avg  \\\n",
       "0  26899.999512  26569.421354  22404.523958  26174.999023  2624.999939   \n",
       "1  26569.421354  22404.523958  26174.999023  21265.799626  2607.673588   \n",
       "2  22404.523958  26174.999023  21265.799626  20437.985954  2604.999939   \n",
       "3  26174.999023  21265.799626  20437.985954  21374.249394  2604.999939   \n",
       "4  21265.799626  20437.985954  21374.249394  19502.152584  2604.999939   \n",
       "\n",
       "       C69_avg      C70_avg      C71_avg  \n",
       "0  2607.673588  2604.999939  2604.999939  \n",
       "1  2604.999939  2604.999939  2604.999939  \n",
       "2  2604.999939  2604.999939  2604.999939  \n",
       "3  2604.999939  2604.999939  2604.999939  \n",
       "4  2604.999939  2604.999939  2604.999939  \n",
       "\n",
       "[5 rows x 431 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fearure_matrix=np.hstack((change_1h,past_1h_matrix))\n",
    "\n",
    "# Generate the column names\n",
    "col_name=[]\n",
    "for i in range(1,72):\n",
    "    col_name.append(\"C\"+str(i)+\"_1h\")\n",
    "    col_name.append(\"C\"+str(i)+\"_45m\")\n",
    "    col_name.append(\"C\"+str(i)+\"_30m\")\n",
    "    col_name.append(\"C\"+str(i)+\"_15m\")\n",
    "for i in range(1,72):\n",
    "    col_name.append(\"C\"+str(i)+\"_change\")\n",
    "for i in range(1,72):\n",
    "    col_name.append(\"C\"+str(i)+\"_avg\")\n",
    "#col_name\n",
    "\n",
    "# Construct feature space dataframe\n",
    "df_feature=pd.DataFrame(fearure_matrix, columns=col_name)\n",
    "df_feature['year_mon']=data_monthwide['year_mon'].values[4:]\n",
    "df_feature['year']=data_monthwide['year'].values[4:]\n",
    "df_feature['month']=data_monthwide['month'].values[4:]\n",
    "df_feature['hour']=data_monthwide['hour'].values[4:]\n",
    "df_feature['minute']=data_monthwide['min'].values[4:]\n",
    "#df_feature['label']=data_monthwide['Label'].values[4:]\n",
    "df_feature=df_feature[[\"year_mon\",\"year\",\"month\",\"hour\",\"minute\"]+col_name]\n",
    "df_feature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Add the weather conditons to the feature space\n",
    "\n",
    "We got the processed data from the R. The weather data was imputed through random forest algorithm. The imputated data called \"weather_imputed.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>apparentTemperature</th>\n",
       "      <th>cloudCover</th>\n",
       "      <th>dewPoint</th>\n",
       "      <th>humidity</th>\n",
       "      <th>icon</th>\n",
       "      <th>precipIntensity</th>\n",
       "      <th>precipProbability</th>\n",
       "      <th>pressure</th>\n",
       "      <th>summary</th>\n",
       "      <th>temperature</th>\n",
       "      <th>time</th>\n",
       "      <th>visibility</th>\n",
       "      <th>windBearing</th>\n",
       "      <th>windSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>55.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>48.68</td>\n",
       "      <td>0.79</td>\n",
       "      <td>clear-night</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1010.21</td>\n",
       "      <td>Clear</td>\n",
       "      <td>55.20</td>\n",
       "      <td>1.467356e+09</td>\n",
       "      <td>8.28</td>\n",
       "      <td>222.0</td>\n",
       "      <td>4.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>55.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>49.47</td>\n",
       "      <td>0.81</td>\n",
       "      <td>clear-night</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1010.05</td>\n",
       "      <td>Clear</td>\n",
       "      <td>55.06</td>\n",
       "      <td>1.467360e+09</td>\n",
       "      <td>8.28</td>\n",
       "      <td>228.0</td>\n",
       "      <td>4.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>54.56</td>\n",
       "      <td>0.15</td>\n",
       "      <td>48.45</td>\n",
       "      <td>0.80</td>\n",
       "      <td>clear-night</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1009.81</td>\n",
       "      <td>Clear</td>\n",
       "      <td>54.56</td>\n",
       "      <td>1.467364e+09</td>\n",
       "      <td>7.94</td>\n",
       "      <td>225.0</td>\n",
       "      <td>5.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>54.62</td>\n",
       "      <td>0.31</td>\n",
       "      <td>49.67</td>\n",
       "      <td>0.83</td>\n",
       "      <td>partly-cloudy-night</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1009.77</td>\n",
       "      <td>Partly Cloudy</td>\n",
       "      <td>54.62</td>\n",
       "      <td>1.467367e+09</td>\n",
       "      <td>8.06</td>\n",
       "      <td>221.0</td>\n",
       "      <td>4.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>54.46</td>\n",
       "      <td>0.15</td>\n",
       "      <td>49.87</td>\n",
       "      <td>0.84</td>\n",
       "      <td>clear-night</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1009.83</td>\n",
       "      <td>Clear</td>\n",
       "      <td>54.46</td>\n",
       "      <td>1.467371e+09</td>\n",
       "      <td>7.62</td>\n",
       "      <td>235.0</td>\n",
       "      <td>3.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  hour  apparentTemperature  cloudCover  dewPoint  humidity  \\\n",
       "0  2016      7     7                55.20        0.00     48.68      0.79   \n",
       "1  2016      7     8                55.06        0.00     49.47      0.81   \n",
       "2  2016      7     9                54.56        0.15     48.45      0.80   \n",
       "3  2016      7    10                54.62        0.31     49.67      0.83   \n",
       "4  2016      7    11                54.46        0.15     49.87      0.84   \n",
       "\n",
       "                  icon  precipIntensity  precipProbability  pressure  \\\n",
       "0          clear-night              0.0                0.0   1010.21   \n",
       "1          clear-night              0.0                0.0   1010.05   \n",
       "2          clear-night              0.0                0.0   1009.81   \n",
       "3  partly-cloudy-night              0.0                0.0   1009.77   \n",
       "4          clear-night              0.0                0.0   1009.83   \n",
       "\n",
       "         summary  temperature          time  visibility  windBearing  \\\n",
       "0          Clear        55.20  1.467356e+09        8.28        222.0   \n",
       "1          Clear        55.06  1.467360e+09        8.28        228.0   \n",
       "2          Clear        54.56  1.467364e+09        7.94        225.0   \n",
       "3  Partly Cloudy        54.62  1.467367e+09        8.06        221.0   \n",
       "4          Clear        54.46  1.467371e+09        7.62        235.0   \n",
       "\n",
       "   windSpeed  \n",
       "0       4.17  \n",
       "1       4.38  \n",
       "2       5.31  \n",
       "3       4.30  \n",
       "4       3.64  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the imputed weather data\n",
    "df_weather=pd.read_csv(\"weather_imputed.csv\")\n",
    "del df_weather['date']\n",
    "df_weather.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check any missing value ---- No\n",
    "df_weather.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>apparentTemperature</th>\n",
       "      <th>cloudCover</th>\n",
       "      <th>dewPoint</th>\n",
       "      <th>humidity</th>\n",
       "      <th>precipIntensity</th>\n",
       "      <th>precipProbability</th>\n",
       "      <th>pressure</th>\n",
       "      <th>temperature</th>\n",
       "      <th>time</th>\n",
       "      <th>visibility</th>\n",
       "      <th>windBearing</th>\n",
       "      <th>windSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>64.600333</td>\n",
       "      <td>0.329333</td>\n",
       "      <td>52.124667</td>\n",
       "      <td>0.646000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1014.185333</td>\n",
       "      <td>64.600333</td>\n",
       "      <td>1.468670e+09</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>259.133333</td>\n",
       "      <td>10.575667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>64.182667</td>\n",
       "      <td>0.249084</td>\n",
       "      <td>52.982000</td>\n",
       "      <td>0.676333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.887333</td>\n",
       "      <td>64.182667</td>\n",
       "      <td>1.468674e+09</td>\n",
       "      <td>9.337000</td>\n",
       "      <td>260.033333</td>\n",
       "      <td>9.641667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>62.015000</td>\n",
       "      <td>0.243764</td>\n",
       "      <td>52.487667</td>\n",
       "      <td>0.714667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.810000</td>\n",
       "      <td>62.015000</td>\n",
       "      <td>1.468678e+09</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>258.333333</td>\n",
       "      <td>9.463667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>59.809667</td>\n",
       "      <td>0.247511</td>\n",
       "      <td>52.555333</td>\n",
       "      <td>0.773000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.907333</td>\n",
       "      <td>59.809667</td>\n",
       "      <td>1.468681e+09</td>\n",
       "      <td>8.985667</td>\n",
       "      <td>251.400000</td>\n",
       "      <td>7.860333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>58.130333</td>\n",
       "      <td>0.241958</td>\n",
       "      <td>52.231000</td>\n",
       "      <td>0.810667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1014.068667</td>\n",
       "      <td>58.130333</td>\n",
       "      <td>1.468685e+09</td>\n",
       "      <td>9.085333</td>\n",
       "      <td>248.066667</td>\n",
       "      <td>7.167000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  hour  apparentTemperature  cloudCover   dewPoint  humidity  \\\n",
       "0  2016      7     0            64.600333    0.329333  52.124667  0.646000   \n",
       "1  2016      7     1            64.182667    0.249084  52.982000  0.676333   \n",
       "2  2016      7     2            62.015000    0.243764  52.487667  0.714667   \n",
       "3  2016      7     3            59.809667    0.247511  52.555333  0.773000   \n",
       "4  2016      7     4            58.130333    0.241958  52.231000  0.810667   \n",
       "\n",
       "   precipIntensity  precipProbability     pressure  temperature          time  \\\n",
       "0              0.0                0.0  1014.185333    64.600333  1.468670e+09   \n",
       "1              0.0                0.0  1013.887333    64.182667  1.468674e+09   \n",
       "2              0.0                0.0  1013.810000    62.015000  1.468678e+09   \n",
       "3              0.0                0.0  1013.907333    59.809667  1.468681e+09   \n",
       "4              0.0                0.0  1014.068667    58.130333  1.468685e+09   \n",
       "\n",
       "   visibility  windBearing  windSpeed  \n",
       "0    9.300000   259.133333  10.575667  \n",
       "1    9.337000   260.033333   9.641667  \n",
       "2    9.100000   258.333333   9.463667  \n",
       "3    8.985667   251.400000   7.860333  \n",
       "4    9.085333   248.066667   7.167000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate monthwide average statistics for each variable at each hour\n",
    "#In this part, we would like to only focus on the numeric values.\n",
    "df_weather_avg=df_weather.groupby(['year','month','hour']).mean()\n",
    "col_name=list(df_weather_avg.columns)\n",
    "weather_num_avg= df_weather_avg.values\n",
    "\n",
    "#generate hour month year index\n",
    "hour=[]\n",
    "for i in range(7):\n",
    "    hour+=range(0,24)\n",
    "month=list(np.repeat([7,8,9,10,11,12,1],24))\n",
    "year=[2016 for x in range(144)]+[2017 for x in range(24)]\n",
    "\n",
    "# construct monthwide weather statistics average dataframe\n",
    "\n",
    "df_weather_avg=pd.DataFrame(weather_num_avg,columns=col_name)\n",
    "df_weather_avg[\"year\"]=year\n",
    "df_weather_avg[\"month\"]=month\n",
    "df_weather_avg[\"hour\"]=hour\n",
    "df_weather_avg=df_weather_avg[[\"year\",\"month\",\"hour\"]+col_name]\n",
    "df_weather_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_mon</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>C1_1h</th>\n",
       "      <th>C1_45m</th>\n",
       "      <th>C1_30m</th>\n",
       "      <th>C1_15m</th>\n",
       "      <th>C2_1h</th>\n",
       "      <th>...</th>\n",
       "      <th>humidity</th>\n",
       "      <th>precipIntensity</th>\n",
       "      <th>precipProbability</th>\n",
       "      <th>pressure</th>\n",
       "      <th>temperature</th>\n",
       "      <th>time</th>\n",
       "      <th>visibility</th>\n",
       "      <th>windBearing</th>\n",
       "      <th>windSpeed</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20160700</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.323747</td>\n",
       "      <td>17.499878</td>\n",
       "      <td>54.053738</td>\n",
       "      <td>37.324973</td>\n",
       "      <td>15.999578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.887333</td>\n",
       "      <td>64.182667</td>\n",
       "      <td>1.468674e+09</td>\n",
       "      <td>9.337</td>\n",
       "      <td>260.033333</td>\n",
       "      <td>9.641667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20160700</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>4.999939</td>\n",
       "      <td>9.633529</td>\n",
       "      <td>366.201780</td>\n",
       "      <td>313.344634</td>\n",
       "      <td>222.080779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.887333</td>\n",
       "      <td>64.182667</td>\n",
       "      <td>1.468674e+09</td>\n",
       "      <td>9.337</td>\n",
       "      <td>260.033333</td>\n",
       "      <td>9.641667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20160700</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>4.999939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>161.285451</td>\n",
       "      <td>145.532020</td>\n",
       "      <td>91.639445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.887333</td>\n",
       "      <td>64.182667</td>\n",
       "      <td>1.468674e+09</td>\n",
       "      <td>9.337</td>\n",
       "      <td>260.033333</td>\n",
       "      <td>9.641667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20160700</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>3.374870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>338.933895</td>\n",
       "      <td>264.666146</td>\n",
       "      <td>179.289524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.887333</td>\n",
       "      <td>64.182667</td>\n",
       "      <td>1.468674e+09</td>\n",
       "      <td>9.337</td>\n",
       "      <td>260.033333</td>\n",
       "      <td>9.641667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20160700</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7.499939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.474115</td>\n",
       "      <td>98.481982</td>\n",
       "      <td>57.822576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.810000</td>\n",
       "      <td>62.015000</td>\n",
       "      <td>1.468678e+09</td>\n",
       "      <td>9.100</td>\n",
       "      <td>258.333333</td>\n",
       "      <td>9.463667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 444 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year_mon  year  month  hour  minute     C1_1h     C1_45m      C1_30m  \\\n",
       "0  20160700  2016      7     1       0  1.323747  17.499878   54.053738   \n",
       "1  20160700  2016      7     1      15  4.999939   9.633529  366.201780   \n",
       "2  20160700  2016      7     1      30  4.999939   0.000000  161.285451   \n",
       "3  20160700  2016      7     1      45  3.374870   0.000000  338.933895   \n",
       "4  20160700  2016      7     2       0  7.499939   0.000000  118.474115   \n",
       "\n",
       "       C1_15m       C2_1h  ...    humidity  precipIntensity  \\\n",
       "0   37.324973   15.999578  ...    0.676333              0.0   \n",
       "1  313.344634  222.080779  ...    0.676333              0.0   \n",
       "2  145.532020   91.639445  ...    0.676333              0.0   \n",
       "3  264.666146  179.289524  ...    0.676333              0.0   \n",
       "4   98.481982   57.822576  ...    0.714667              0.0   \n",
       "\n",
       "   precipProbability     pressure  temperature          time  visibility  \\\n",
       "0                0.0  1013.887333    64.182667  1.468674e+09       9.337   \n",
       "1                0.0  1013.887333    64.182667  1.468674e+09       9.337   \n",
       "2                0.0  1013.887333    64.182667  1.468674e+09       9.337   \n",
       "3                0.0  1013.887333    64.182667  1.468674e+09       9.337   \n",
       "4                0.0  1013.810000    62.015000  1.468678e+09       9.100   \n",
       "\n",
       "   windBearing  windSpeed  label  \n",
       "0   260.033333   9.641667      0  \n",
       "1   260.033333   9.641667      0  \n",
       "2   260.033333   9.641667      0  \n",
       "3   260.033333   9.641667      0  \n",
       "4   258.333333   9.463667      0  \n",
       "\n",
       "[5 rows x 444 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join the weather condition dataframe with the previous feature data frame \n",
    "# on year, month and hour\n",
    "\n",
    "df_feature=pd.merge(df_feature,df_weather_avg, on=[\"year\",\"month\",\"hour\"],how='left' )\n",
    "df_feature['label']=data_monthwide['Label'].values[4:]\n",
    "\n",
    "# Final feature space\n",
    "df_feature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Model building\n",
    "\n",
    "## First Glance and Basic Ideas\n",
    "\n",
    "This part will be divided into 2 parts.The first part would be explore the **supervised** learning solutions to try to explore optimal classifier to predict the peak. The second part would be to try to implement **unsupervised** solutions. We will try to fit data on both the whole dataset and the top 50 most important feature dataset. \n",
    "\n",
    "## Main Challenge\n",
    "As we can see, since the peak is the largest average energy use over 15 min interval by month. So there would be only one peak at each month. Since there are seven month in our data, so there will be only 7 peak data in our datasets. The dataset is highly imbalanced. So I came up with three models that I want to try. **The first one is Adaboost and then weighted logistic regression and also one-class SVM.**\n",
    "\n",
    "## Model Assumptions:\n",
    "1. The system of this model is to use the 1 hour before data to predict whether there will be a peak in a 15 min time period. \n",
    "2. The time in our model should be encoded as a 15-min time period. For example, this model could predict whether there will be peak during 4:00am to 4:15am.\n",
    "3. The weather data for a specific hour should be obtained beforehand. That is, the data in our feature space should all known data.\n",
    "\n",
    "## Performance measure\n",
    "** Confusion Matrix **: Confusion matrix is one way to evaluate performance of the the classifer. It is a table that used to display whether the system mislabelled each class or confused two classes. The rows and the columns are all the categories of the response variable. However, the columns are under the prediction condition and the rows are under the actual conditions. \n",
    "\n",
    "| Predicted/ Actual | Class 0         | Class 1        |\n",
    "|-------------------|-----------------|----------------|\n",
    "| Class 0           | True Negatives  | False Negative |\n",
    "| Class 1           | False Positives | True Positives |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost\n",
    "\n",
    "The **reason** that we want to choose adaboost is that it is a ensemble method. And most importantly, it is a **cost sensitive method**. Adaboost will start from a weak learner, say, stump tree. And subsequently add weak learners to the ensembles to build a strong learner. The weight vector will be adjusted according to the points that are missclassified. \n",
    "\n",
    "## Whole datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "# First, we need to split our peaks randomly into train and test\n",
    "# Since the data is highly imbalanced data and the number of peaks are very small so we have to split it first\n",
    "x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(df_feature[df_feature['label']==1].ix[:,:-1].values, df_feature[df_feature['label']==1].ix[:,-1].values, test_size=0.3)\n",
    "\n",
    "#Then we split the normal data, whose labels are 0\n",
    "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(df_feature[df_feature['label']==0].ix[:,:-1].values, df_feature[df_feature['label']==0].ix[:,-1].values, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_ib=np.concatenate((y_train_1, y_train_2), axis=0)\n",
    "x_train_ib=np.concatenate((x_train_1, x_train_2), axis=0)\n",
    "y_test_ib=np.concatenate((y_test_1, y_test_2), axis=0)\n",
    "x_test_ib=np.concatenate((x_test_1, x_test_2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75711892797319935"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada=AdaBoostClassifier()\n",
    "ada.fit(x_train_ib,y_train_ib)\n",
    "y_pred=ada.predict_proba(x_test_ib)\n",
    "y_score=y_pred[:,1]\n",
    "y_p_ada=ada.predict(x_test_ib)\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test_ib, y_score)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>199</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  199  0\n",
       "1    3  0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build confusion matrix\n",
    "cm_ada=metrics.confusion_matrix(y_test_ib,y_p_ada)\n",
    "df_cm_ada= pd.DataFrame(cm_ada, columns=['0','1'])\n",
    "df_cm_ada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 50 most important feature dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calsulate the most important features score by Random Forest\n",
    "X=df_feature.ix[:,:-1].values\n",
    "y=df_feature.ix[:,-1].values\n",
    "forest = ExtraTreesClassifier(n_estimators=250,random_state=0)\n",
    "forest.fit(X, y)\n",
    "importances = forest.feature_importances_\n",
    "df_feature_importance=pd.DataFrame({'Feature':list(df_feature.columns)[:-1], 'Importance':importances})\n",
    "df_feature_importance=df_feature_importance.sort_values(by='Importance',ascending=0)\n",
    "\n",
    "feature_50=list(df_feature_importance.ix[:51,0].values)\n",
    "df_feature_50=df_feature[feature_50+['label']]\n",
    "\n",
    "# Split the data into train and test\n",
    "# First, we need to split our peaks randomly into train and test\n",
    "# Since the data is highly imbalanced data and the number of peaks are very small so we have to split it first\n",
    "x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(df_feature_50[df_feature_50['label']==1].ix[:,:-1].values, df_feature_50[df_feature_50['label']==1].ix[:,-1].values, test_size=0.3)\n",
    "\n",
    "#Then we split the normal data, whose labels are 0\n",
    "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(df_feature_50[df_feature_50['label']==0].ix[:,:-1].values, df_feature_50[df_feature_50['label']==0].ix[:,-1].values, test_size=0.3)\n",
    "\n",
    "y_train_50=np.concatenate((y_train_1, y_train_2), axis=0)\n",
    "x_train_50=np.concatenate((x_train_1, x_train_2), axis=0)\n",
    "y_test_50=np.concatenate((y_test_1, y_test_2), axis=0)\n",
    "x_test_50=np.concatenate((x_test_1, x_test_2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79396984924623115"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada=AdaBoostClassifier()\n",
    "ada.fit(x_train_50,y_train_50)\n",
    "y_pred=ada.predict_proba(x_test_50)\n",
    "y_score=y_pred[:,1]\n",
    "y_p_ada=ada.predict(x_test_50)\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test_50, y_score)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  198  1\n",
       "1    3  0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build confusion matrix\n",
    "cm_ada=metrics.confusion_matrix(y_test_50,y_p_ada)\n",
    "df_cm_ada= pd.DataFrame(cm_ada, columns=['0','1'])\n",
    "df_cm_ada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result analysis:\n",
    "\n",
    "As we can see from the confusion matrix, it fails to predict all the peaks in the test set for both the whole dataset and 50-feature dataset. Peaks are the main concern in our problem. So I believe Adaboost is not a good method here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74204355108877718"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlr=LogisticRegression(penalty='l1',class_weight='balanced')\n",
    "wlr.fit(x_train_ib,y_train_ib)\n",
    "y_pred=wlr.predict_proba(x_test_ib)\n",
    "y_score=y_pred[:,1]\n",
    "y_p_lr=wlr.predict(x_test_ib)\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test_ib, y_score)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>199</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  199  0\n",
       "1    3  0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_lr=metrics.confusion_matrix(y_test_ib,y_p_lr)\n",
    "df_cm_lr= pd.DataFrame(cm_lr, columns=['0','1'])\n",
    "df_cm_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6649916247906198"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlr=LogisticRegression(penalty='l1',class_weight='balanced')\n",
    "wlr.fit(x_train_50,y_train_50)\n",
    "y_pred=wlr.predict_proba(x_test_50)\n",
    "y_score=y_pred[:,1]\n",
    "y_p_lr=wlr.predict(x_test_50)\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test_50, y_score)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>193</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  193  6\n",
       "1    2  1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_lr=metrics.confusion_matrix(y_test_50,y_p_lr)\n",
    "df_cm_lr= pd.DataFrame(cm_lr, columns=['0','1'])\n",
    "df_cm_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result analysis:\n",
    "\n",
    "As we can see from the confusion matrix, it fails to predict all the peaks in the test set for both the whole dataset and 50-feature dataset. Peaks are the main concern in our problem. So I believe weighted logistic regression is not a good method here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-class SVM\n",
    "\n",
    "One-class SVM is widely applied in time-series anomaly detection. It is a unsupervised learning method. It assumes that all training sets have the same labels. This classifier will learn a soft boundary for the normal dataset. Any observations that did not fall into the soft boundary will be declared as anomaly. One can choose different kernels, such as RBF, polynomial and linear kernels. Here in our project, I found that **Linear Kernel** outperformed other kernels.\n",
    "\n",
    "### Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma='auto',\n",
       "      kernel='linear', max_iter=-1, nu=0.5, random_state=None,\n",
       "      shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.OneClassSVM(kernel=\"linear\")\n",
    "clf.fit(x_train_ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  1.,  0.,  1.,\n",
       "        0.,  0.,  1.,  0.,  1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,\n",
       "        1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,\n",
       "        0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,\n",
       "        0.,  1.,  1.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,\n",
       "        0.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,\n",
       "        0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,\n",
       "        1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  1.,\n",
       "        1.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  1.,\n",
       "        0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,\n",
       "        1.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "        0.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  1.,  1.,  0.,  1.,  0.,\n",
       "        1.,  0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  1.,  0.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = clf.predict(x_test_ib)\n",
    "y_pred_test[y_pred_test==-1]=0\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_ib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  105  94\n",
       "1    2   1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion matrix\n",
    "cm_oneSVM=metrics.confusion_matrix(y_test_ib,y_pred_test)\n",
    "df_cm_oneSVM= pd.DataFrame(cm_oneSVM, columns=['0','1'])\n",
    "df_cm_oneSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50-Feature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma='auto',\n",
       "      kernel='linear', max_iter=-1, nu=0.5, random_state=None,\n",
       "      shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.OneClassSVM(kernel=\"linear\")\n",
    "clf.fit(x_train_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,\n",
       "        1.,  1.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,\n",
       "        0.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,\n",
       "        1.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  0.,  0.,  1.,\n",
       "        1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,\n",
       "        0.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,  0.,  1.,  1.,  1.,  0.,\n",
       "        0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,\n",
       "        1.,  1.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,\n",
       "        0.,  1.,  1.,  0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "        1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  1.,  0.,\n",
       "        1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "        1.,  1.,  1.,  0.,  1.,  0.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
       "        0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "        1.,  0.,  0.,  0.,  0.,  1.,  1.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = clf.predict(x_test_50)\n",
    "y_pred_test[y_pred_test==-1]=0\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  104  95\n",
       "1    0   3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion matrix\n",
    "cm_oneSVM=metrics.confusion_matrix(y_test_50,y_pred_test)\n",
    "df_cm_oneSVM= pd.DataFrame(cm_oneSVM, columns=['0','1'])\n",
    "df_cm_oneSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis:\n",
    "We can find that one-class SVM trained on the most important 50 feature space successfully detected all the peaks on the test (Unseen) dataset. So we conclude that **one-class SVM outperformed other models and could be our final model on predicting the monthly peak energy use**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusion:\n",
    "\n",
    "We found that **One-Class SVM** trained on the 50 most important feature space performs pretty well on predicting the peaks for the future. What one-class SVM does is that it assumes that all training sets have the same labels. This classifier will learn a soft boundary for the normal dataset. Any observations that did not fall into the soft boundary will be declared as peaks or anomalys. \n",
    "\n",
    "However, there are still some problems in this final model. This model still does not perform pretty well on predicting normal points. According to the customer's definition, if they could find peaks in advance, they could take preventative measures. If the cost of the preventative measures do not have very high cost, then this model is a pretty good model because it could detect all the peaks and could help the customer save a lot of money. However, if taking the preventative measures will cost a lot, then this model may not be an appropriate option. In this case, we may want to collect more data to train our model. Or we may want to resample our highly imbalanced data by methods like **bootstrapping**. Through bootstrapping, we can resampe more abnomal points(say peaks) and re-fit all the models above to see whether there are any improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
